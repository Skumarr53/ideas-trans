{
  "name": "Daily Sentiment Inference",
  "schedule": {
    "quartz_cron_expression": "0 0 1 * * ?",
    "timezone_id": "UTC",
    "pause_status": "UNPAUSED"
  },
  "tasks": [
    {
      "task_key": "LoadSnowflake",
      "notebook_task": {
        "notebook_path": "/Repos/Team/BatchJobs/Load_From_Snowflake",
        "source": "WORKSPACE"
      },
      "cluster_key": "shared_cluster",
      "description": "Read new data from Snowflake into Spark"
    },
    {
      "task_key": "RunInference",
      "depends_on": [{ "task_key": "LoadSnowflake" }],
      "python_task": {
        "python_file": "dbfs:/FileStore/code/batch_inference.py",
        "parameters": [
          "--model-name", "sentiment_model_v2",
          "--input-table", "<SNOWFLAKE_TABLE>",
          "--output-temp", "/tmp/output.parquet"
        ]
      },
      "cluster_key": "adaptive_cluster",
      "description": "Apply sentiment model to data"
    },
    {
      "task_key": "WriteOutput",
      "depends_on": [{ "task_key": "RunInference" }],
      "notebook_task": {
        "notebook_path": "/Repos/Team/BatchJobs/Write_To_Snowflake",
        "source": "WORKSPACE",
        "base_parameters": {
          "output_table": "<SNOWFLAKE_OUTPUT_TABLE>"
        }
      },
      "cluster_key": "shared_cluster",
      "description": "Write results back to Snowflake"
    }
  ],
  "job_clusters": [
    {
      "job_cluster_key": "shared_cluster",
      "new_cluster": {
        "spark_version": "16.4.x-scala2.12",
        "node_type_id": "standard_ds3_v2",
        "num_workers": 2,
        "custom_tags": { "usage": "sentiment-batch" },
        "spark_conf": {
          "spark.databricks.cluster.profile": "singleNode",
          "spark.databricks.pyspark.pythonVersion": "3"
        },
        "docker_image": {
          "url": "my-org/sentiment-agent:latest",
          "basic_auth": {
            "username": "{{secrets/container_repo/user}}",
            "password": "{{secrets/container_repo/pwd}}"
          }
        }
      }
    },
    {
      "job_cluster_key": "adaptive_cluster",
      "new_cluster": {
        "spark_version": "16.4.x-scala2.12",
        "node_type_id": "standard_ds4_v2",
        "num_workers": 4,
        "custom_tags": { "usage": "model-sensitive-batch" },
        "spark_conf": {
          "spark.sql.shuffle.partitions": "200",
          "spark.databricks.pyspark.pythonVersion": "3"
        },
        "docker_image": {
          "url": "my-org/sentiment-agent:latest",
          "basic_auth": {
            "username": "{{secrets/container_repo/user}}",
            "password": "{{secrets/container_repo/pwd}}"
          }
        }
      }
    }
  ],
  "email_notifications": {
    "on_failure": ["data-team@datacompany.com"]
  }
}
